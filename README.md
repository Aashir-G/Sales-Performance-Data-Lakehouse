```md
# ğŸ“Š Sales Performance Data Lakehouse

An end-to-end analytics pipeline that transforms raw business data into executive-level dashboards using a modern **Data Lakehouse architecture**.

Pipeline:
```

Excel / CSV â†’ Python â†’ Databricks (Delta Lake) â†’ Power BI

```

This project shows how messy operational data is engineered into scalable, analytics-ready tables and consumed by BI tools.

---

## ğŸ—ï¸ Architecture

```

Raw Excel / CSV
â†“
Python (Cleaning + Feature Engineering)
â†“
Databricks Lakehouse
â”œâ”€ Bronze Layer  â†’ Raw ingestion
â”œâ”€ Silver Layer  â†’ Cleaned & typed data
â””â”€ Gold Layer    â†’ Aggregated KPIs for BI
â†“
Power BI Dashboard (Executive Overview)

```

---

## ğŸ§° Tech Stack

| Tool | Purpose |
|------|-------|
| Python (Pandas) | Data cleaning and preprocessing |
| Databricks | Distributed compute + Lakehouse platform |
| PySpark | Transformations & aggregations |
| Delta Lake | Reliable table storage (ACID) |
| Power BI | Data visualization & dashboards |
| Excel | Raw data source |
| GitHub | Version control |

---

## ğŸ“ Project Structure

```

Sales-Performance-Data-Lakehouse/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Generated raw Excel/CSV (git ignored)
â”‚   â”œâ”€â”€ processed/           # Cleaned CSV output (git ignored)
â”‚   â””â”€â”€ sample/              # Small sample dataset for demo
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ lakehouse_pipeline.py   # Bronze â†’ Silver â†’ Gold pipeline
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generate_raw_excel.py   # Fake dataset generator
â”‚   â”œâ”€â”€ clean_excel_to_csv.py   # Cleaning + feature engineering
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ dashboard.png
â”‚   â””â”€â”€ databricks_tables.png
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ”„ Data Layers

### ğŸ¥‰ Bronze Layer â€“ Raw Ingestion
Stores unmodified data for lineage and recovery.

```python
df_bronze = spark.table("workspace.default.silver_sales")
df_bronze.write.format("delta").saveAsTable("bronze_sales")
````

---

### ğŸ¥ˆ Silver Layer â€“ Curated Data

* Type casting
* Null handling
* Data quality filters
* Standardized schema

```python
df_silver.write.format("delta").saveAsTable("silver_sales_curated")
```

---

### ğŸ¥‡ Gold Layer â€“ Analytics Tables

| Table             | Description                   |
| ----------------- | ----------------------------- |
| gold_kpis         | Daily KPIs by channel         |
| gold_product_perf | Product-level performance     |
| gold_region_perf  | Region-level revenue & profit |

Only these tables are used by Power BI.

---

## ğŸ“Š KPIs Implemented

* Net Revenue
* Profit (estimated with COGS)
* ROAS (Return on Ad Spend)
* AOV (Average Order Value)
* Return Rate
* Orders
* Unique Customers

---

## ğŸ“ˆ Power BI Dashboard

Page: **Executive Overview**

Contains:

* KPI Cards:

  * Net Revenue
  * Profit
  * ROAS
  * Orders

* Visuals:

  * Revenue Trend (Line Chart)
  * Revenue by Channel (Column Chart)
  * Top Products (Bar Chart)
  * Revenue by Region (Bar Chart)

* Slicers:

  * Date range
  * Channel
  * Region

---

## ğŸ–¼ï¸ Dashboard Preview

```md
![alt text](image-1.png)
```

---

## ğŸ—ƒï¸ Databricks Tables Preview

```md
![alt text](image.png)
```

---

## ğŸš€ Run Locally (Python)

```bash
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt

# Generate raw dataset
python -m src.generate_raw_excel

# Clean Excel â†’ CSV
python -m src.clean_excel_to_csv
```

---

## â˜ï¸ Run in Databricks

1. Upload cleaned CSV using:

   * `Create or modify table from file upload`
2. Run the notebook:

```
notebooks/lakehouse_pipeline.py
```

3. Verify tables:

```python
spark.sql("SHOW TABLES").show()
```

You should see:

```
bronze_sales
silver_sales_curated
gold_kpis
gold_product_perf
gold_region_perf
```

---

## ğŸ”Œ Power BI Connection

Power BI â†’ Get Data â†’ **Azure Databricks**

Use:

* Server hostname
* HTTP Path
* Personal Access Token

Load:

* gold_kpis
* gold_product_perf
* gold_region_perf

---

## ğŸ“Œ Notes

* Only Gold tables are exposed to BI tools
* Bronze & Silver layers stay internal to Databricks
* Delta Lake ensures schema consistency and reliability
* Architecture matches real enterprise analytics pipelines

---

## ğŸ”¥ Project Outcome

This project implements a full **Lakehouse analytics workflow** with:

* Raw data ingestion
* Structured transformation layers
* Production-style KPI tables
* Live BI dashboard integration

