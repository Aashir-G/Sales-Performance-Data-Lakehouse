# Sales Performance Data Lakehouse ğŸ“Š

An end-to-end analytics project that simulates an enterprise-grade **Lakehouse architecture** using Databricks + Power BI.
It ingests raw sales data, transforms it through Bronze â†’ Silver â†’ Gold layers, and delivers clean KPI-driven dashboards for business decision making.

![Databricks](https://img.shields.io/badge/Databricks-FF3621?logo=databricks\&logoColor=white)
![Power BI](https://img.shields.io/badge/Power%20BI-F2C811?logo=powerbi\&logoColor=black)
![PySpark](https://img.shields.io/badge/PySpark-E25A1C?logo=apache-spark\&logoColor=white)
![Lakehouse](https://img.shields.io/badge/Lakehouse-Architecture-4B8BBE)

---

## Features âœ¨

### ğŸ—ï¸ Medallion Architecture

Implements a professional Lakehouse pipeline:

* **Bronze Layer** â€“ Raw ingested data
* **Silver Layer** â€“ Cleaned and structured data
* **Gold Layer** â€“ Aggregated KPIs and analytics tables

```
UI Upload
â†’ Silver Table
â†’ Bronze Table
â†’ Silver Table (refined)
â†’ Gold Tables (analytics ready)
```

Gold tables:

* `gold_kpis`
* `gold_product_perf`
* `gold_region_perf`

---

### ğŸ“ˆ Revenue Analytics

* Year-over-year revenue trends
* Channel-wise revenue distribution
* Business-ready KPIs for leadership reporting

---

### ğŸ§® Business Metrics

Calculated metrics include:

* Total Revenue
* Average Order Value (AOV)
* Order Count
* Revenue by Channel
* Revenue by Region
* Product Performance

---

### ğŸ“Š Power BI Dashboard

Interactive dashboard features:

* Revenue trend visualization
* Channel performance bar charts
* Date range filtering (Between slicer)
* Enterprise-style layout
* Clean executive reporting design

---

### âš™ï¸ Enterprise-Style Data Pipeline

* PySpark transformations
* Delta tables
* SQL Warehouse / Cluster integration
* Power BI Direct connection via Databricks

This mirrors how real companies structure their analytics stacks.

---

## Architecture Overview ğŸ›ï¸

```
CSV Upload
   â†“
Databricks FileStore (DBFS)
   â†“
Bronze Table (Raw)
   â†“
Silver Table (Cleaned)
   â†“
Gold Tables (Aggregations)
   â†“
Power BI Dashboard
```

---

## File Structure ğŸ“

```
Sales-Performance-Data-Lakehouse/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ silver_sales.csv
â”œâ”€â”€ databricks/
â”‚   â””â”€â”€ lakehouse_pipeline.py   # Full PySpark pipeline
â”œâ”€â”€ powerbi/
â”‚   â””â”€â”€ sales_dashboard.pbix    # Power BI report
â”œâ”€â”€ README.md
```

---

## Technologies Used ğŸ› ï¸

* **Databricks** â€“ Distributed processing & Lakehouse storage
* **PySpark** â€“ Data transformations and aggregations
* **Delta Lake** â€“ Transactional tables
* **Power BI** â€“ Visualization & analytics
* **SQL Warehouse** â€“ BI connectivity
* **DBFS** â€“ File ingestion

---

## Setup & Execution ğŸš€

### 1. Upload Data

Upload the CSV file:

```
/FileStore/tables/silver_sales.csv
```

---

### 2. Run the Pipeline in Databricks

Create a new Python notebook and paste the pipeline code.

Attach a cluster and run:

```python
spark.sql("SHOW TABLES").show()
```

You should see:

```
bronze_sales
silver_sales
gold_kpis
gold_product_perf
gold_region_perf
```

Preview:

```python
display(spark.table("gold_kpis"))
display(spark.table("gold_product_perf"))
display(spark.table("gold_region_perf"))
```

---

### 3. Connect Power BI

In Power BI:

1. Get Data â†’ Azure â†’ Azure Databricks
2. Enter:

   * Server Hostname
   * HTTP Path
   * Authentication: Personal Access Token
3. Select:

   * `gold_kpis`
   * `gold_product_perf`
   * `gold_region_perf`

---

## Sample Insights ğŸ“Š

| KPI                  | Description                    |
| -------------------- | ------------------------------ |
| Revenue Trend        | Tracks revenue over time       |
| Channel Revenue      | Breakdown by marketing channel |
| Product Performance  | Top-selling items              |
| Regional Performance | Strongest geographic markets   |

---

## Why This Project Matters ğŸ¯

This project demonstrates:

* Real enterprise analytics architecture
* Data engineering + BI integration
* Production-style pipelines
* Recruiter-ready portfolio quality

Easy explanation in interviews:

> â€œI built a full Databricks Lakehouse that processes raw sales data into business KPIs and visualized everything in Power BI using a professional analytics pipeline.â€

---

## Future Improvements ğŸ’­

* [ ] Real-time streaming ingestion
* [ ] Incremental Delta updates
* [ ] Data quality checks
* [ ] CI/CD deployment
* [ ] Cost optimization layer
* [ ] ML forecasting on revenue

---

## License ğŸ“„

MIT License

---

## Support ğŸ’¬

Have ideas or feedback?

* Open an issue
* Submit a PR
* Or build on top of it

---

**Built to simulate real enterprise analytics systems.**
Lakehouse thinking. BI execution. Career-level portfolio project ğŸš€
